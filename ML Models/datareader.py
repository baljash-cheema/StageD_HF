import pandas as pd
import random
import dataclean_utils as dc
import torch


def read(filename, path='./data/'):
    '''
    basic function to read a csv file from path (default ./data/)
    '''
    csv = pd.read_csv(path + filename)
    return csv


def split_hyperparams_target(data, targetvar):
    '''
    returns the hyperparameters separated from the target inside data
    '''

    targetvar_cols = [col for col in data.columns if col[:len(targetvar)] == targetvar]
    target = data[targetvar_cols]
    hparams = data.drop(columns=targetvar_cols)
    return hparams, target


def read_cont(dropna=False):
    '''
    read the continuous data
    :param: dropna: True if we drop samples containing any NaN term
    '''
    filename = "deid_full_data_cont.csv"
    data = read(filename)
    if dropna:
        data = data.dropna(axis=0)
    hparams, target = split_hyperparams_target(data, "stage")
    return (hparams, target)


def read_cat(dropna=False):
    '''
    read the categorical data
    :param: dropna: True if we drop samples containing any NaN term
    '''
    filename = "deid_full_data_cat.csv"
    data = read(filename)
    if dropna:
        data = data.dropna(axis=0)
    data = data.drop(columns='id')
    hparams, target = split_hyperparams_target(data, "stage_category_int")
    hparams = dc.explode(hparams)
    return (hparams, target)

def read_cat_test(dropna=False):
    '''
    read the categorical data from test set
    :param: dropna: True if we drop samples containing any NaN term
    '''
    filename = "manual_review_cat.csv"
    data = read(filename)
    if dropna:
        data = data.dropna(axis=0)
    data = data.drop(columns='id')
    hparams, target = split_hyperparams_target(data, "stage_category_int")
    hparams = dc.explode(hparams)
    return (hparams, target)

def read_linsep():
    '''
    read a dummy set of data I've generated by hand that we would expect ~100% accuracy on.
    To debug our models if needed
    '''

    data = pd.DataFrame(columns=['var1','var2','target'],index=range(100))
    for i in data.index:
        a = random.uniform(0,1)
        b = random.uniform(0,1)
        data.loc[i,'var1'] = a
        data.loc[i,'var2'] = b
        data.loc[i,'target'] = [1 if a+b > 1 else 0][0]
    hparams, target = split_hyperparams_target(data, "target")
    hparams = hparams.astype('float')
    target = target.astype('int')
    return (hparams, target)


def generate_sets(data, splits = [70,15,15]):
    '''
    takes data, and splits into subsets
    :param: data = list of 2 dataframes. first dataframe is hyperparams, 2nd is target. must be same length
    :param splits = list of how much % of data we want in each subset
    :returns N lists of data, one for each split. each list contains 2 pd dataframes [hparams, target]
    '''

    assert(sum(splits)==100)

    split_data = []
    _prior = 0
    for split in splits:
        _n = _prior + int(len(data[0])*split/100.)
        subset = [torch.tensor(d.iloc[_prior:_n].values).to(torch.float32) for d in data]
        split_data.append(subset)
        _prior = _n

    return split_data
